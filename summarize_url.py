import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from urllib.parse import urljoin
from gtts import gTTS
from dotenv import load_dotenv
import os
load_dotenv()

def get_links_from_page(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    html = requests.get(url, headers=headers).text
    soup = BeautifulSoup(html, "html.parser")
    
    a_tags = soup.find_all("a", class_="article__item", href=True)
    print(f"å…±æ‰¾åˆ° {len(a_tags)} å€‹æ–‡ç« é€£çµ")
    
    links = []
    for a in a_tags:
        href = a['href']
        full_url = urljoin(url, href)  # æŠŠç›¸å°è·¯å¾‘è£œæˆå®Œæ•´ç¶²å€
        links.append(full_url)
    
    return links

def get_article_text(url):
    soup = BeautifulSoup(requests.get(url).text, "html.parser")
    return soup.get_text()

def generate_summary(text, persona="general"):
    prompt = f"è«‹é‡å°ä»¥ä¸‹æ–‡ç« å…§å®¹ï¼Œç‚ºä¸€ä½ã€Œ{persona}ã€ç”¢å‡ºæ‘˜è¦ï¼Œæ§åˆ¶åœ¨ 300 å­—å…§ï¼š\n{text[:3000]}"
    print(f"\n{prompt}\n")
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

def summarize_index(url, persona="ä¸€èˆ¬ç”¨æˆ¶"):
    links = get_links_from_page(url)
    print(f"æ‰¾åˆ° {len(links)} å€‹æ–‡ç« é€£çµã€‚")
    if not links:
        return []   
    else:
        print("æ–‡ç« é€£çµï¼š", links)
    
    # ç”Ÿæˆå‰10ç¯‡æ‘˜è¦
    first_ten_links = links[:10]
    print(f"ğŸ” æ­£åœ¨ç‚º {len(first_ten_links)} ç¯‡æ–‡ç« ç”Ÿæˆæ‘˜è¦")
    
    summaries = []
    for idx, link in enumerate(first_ten_links, 1):
        text = get_article_text(link)
        summary = generate_summary(text, persona)
        print(f"è™•ç† {link} ...")
        print(f"æ‘˜è¦ï¼š{summary}")
        summaries.append({"url": link, "summary": summary})
        # ç›´æ¥åœ¨é€™è£¡å¯«å…¥æ‘˜è¦æª”æ¡ˆ
        filename = f"summaries/summary_{persona}_{idx}.txt"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"URL: {link}\næ‘˜è¦: {summary}\n")
    return summaries





if __name__ == "__main__":
    url = "https://supertaste.tvbs.com.tw/asia/"
    print(f"ğŸ” æ­£åœ¨è™•ç†ç¶²ç«™ï¼š{url}")
    persona_list = ["ä¸€èˆ¬ç”¨æˆ¶", "å¹´è¼•å¥³æ€§", "å¹´è¼•ç”·æ€§", "ä¸­å¹´å¥³æ€§", "ä¸­å¹´ç”·æ€§", "é«˜é½¡å¥³æ€§", "é«˜é½¡ç”·æ€§"]
    for persona in persona_list:
        print(f"ğŸ” æ­£åœ¨ç‚º {persona} ç”Ÿæˆæ‘˜è¦...")
        results = summarize_index(url, persona=persona)
    print("âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆï¼")
  
  